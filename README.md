# Improved Reuters Classification Model

## Description

This repository contains an enhanced version of a Reuters classification model implemented using Keras. The model's architecture and training process have been refined to improve performance in terms of accuracy and validation accuracy. Adjustments include modifications to the model architecture, such as the addition of dropout layers, changes in batch sizes, and incorporation of batch normalization techniques. The primary goal was to achieve higher accuracy and validation accuracy compared to the original model while maintaining or improving efficiency.

## Goals

- Enhance model accuracy and validation accuracy.
- Implement architectural modifications to boost performance.
- Evaluate the impact of changes like dropout layers, batch sizes, and batch normalization.
- Attain superior performance metrics compared to the original model.
- Maintain or improve efficiency in terms of training time and computational resources.
- Provide clear visualizations of training and validation metrics for comparison.
- Assess model performance using confusion matrices and other relevant metrics.

## Outcome

The improved model outperformed the original model in both accuracy and validation accuracy by the 20th epoch. Despite starting with lower accuracy metrics, the refined model achieved an accuracy of 96.29% and a validation accuracy of 81.50%, surpassing the original model's 95.95% accuracy and 81.10% validation accuracy. Notably, these improvements were accomplished with a reduced batch size of 128, indicating enhanced efficiency without sacrificing performance. Additionally, visualizations of training and validation metrics, along with confusion matrices, were provided to facilitate performance evaluation. Overall, the enhanced model demonstrated superior performance metrics and efficiency compared to the original model.

## Usage
1. Clone the repository.
2. Open `Project06_Neural_Networks_for_Tabular_Data.ipynb` script file and execute it using integrated development environment (IDE)
like Jupyter Notebook or Colaboratory. And upload Dataset into that platform so that it could initiate properly. !Notice
uploaded dataset into platform will be available only for 24 hours after that you will need to upload it again into that
platform.
